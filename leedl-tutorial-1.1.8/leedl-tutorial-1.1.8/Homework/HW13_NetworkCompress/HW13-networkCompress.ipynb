{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Homework 13 - 神经网络压缩(`Network Compression`)\n","\n","作者: Liang-Hsuan Tseng (b07502072@ntu.edu.tw), modified from ML2021-HW13  \n","如果你有任何问题, 可以免费询问: ntu-ml-2022spring-ta@googlegroups.com  \n","\n","[**HW13 PPT**](https://docs.google.com/presentation/d/1nCT9XrInF21B4qQAWuODy5sonKDnpGhjtcAwqa75mVU/edit#slide=id.p)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## noteBook 目录\n","\n","* [Packages](#Packages) - 安转必要的一些包\n","* [Configs](#Configs) - 实验的配置，你可以在这里更改一些超参数。\n","* [Dataset](#Dataset) - 您需要了解的有关数据集的信息。\n","* [Architecture_Design](#Architecture_Design) - 深度(`depthwise`)和逐点(`pointwise`)卷积示例以及一些有用的链接。  \n","* [Knowledge_Distillation](#Knowledge_Distillation) - 在知识提炼中的KL离散损失和一些有用的链接。\n","* [Training](#Training) - 从HW3修改的训练循环实现。\n","* [Inference](#Inference) - 用训练产出的`student_best.ckpt`生成`submission.csv` 。"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:51:04.759191Z","iopub.status.busy":"2023-07-02T08:51:04.758909Z","iopub.status.idle":"2023-07-02T08:51:18.253206Z","shell.execute_reply":"2023-07-02T08:51:18.251989Z","shell.execute_reply.started":"2023-07-02T08:51:04.759168Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torchsummary\n","  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n","Installing collected packages: torchsummary\n","Successfully installed torchsummary-1.5.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install torchsummary"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-02T08:51:18.257476Z","iopub.status.busy":"2023-07-02T08:51:18.257123Z","iopub.status.idle":"2023-07-02T08:51:22.434050Z","shell.execute_reply":"2023-07-02T08:51:22.432778Z","shell.execute_reply.started":"2023-07-02T08:51:18.257446Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sun Jul  2 08:51:22 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from PIL import Image\n","from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset # \"ConcatDataset\" 和 \"Subset\" 有可能使用\n","from torchvision.datasets import DatasetFolder, VisionDataset\n","from torchsummary import summary\n","# from tqdm.auto import tqdm\n","from tqdm import tqdm\n","import random\n","\n","# 查看GPU\n","!nvidia-smi"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Configs\n","\n","在本部分中，你可以指定一些变量和超参数作为配置。"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:51:22.437419Z","iopub.status.busy":"2023-07-02T08:51:22.436049Z","iopub.status.idle":"2023-07-02T08:51:22.446568Z","shell.execute_reply":"2023-07-02T08:51:22.445527Z","shell.execute_reply.started":"2023-07-02T08:51:22.437380Z"},"trusted":true},"outputs":[],"source":["def all_seed(seed=6666, env=None):\n","    if env is not None:\n","        env.seed(seed)\n","        env.action_space.seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    # CPU\n","    torch.manual_seed(seed)\n","    # GPU\n","    if torch.cuda.is_available():\n","        \n","        torch.cuda.manual_seed_all(seed)\n","        torch.cuda.manual_seed(seed)\n","    # python全局\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    # cudnn\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.enabled = False\n","    print(f'Set env random_seed = {seed}')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:53:47.954302Z","iopub.status.busy":"2023-07-02T08:53:47.953895Z","iopub.status.idle":"2023-07-02T08:53:47.960245Z","shell.execute_reply":"2023-07-02T08:53:47.958967Z","shell.execute_reply.started":"2023-07-02T08:53:47.954273Z"},"trusted":true},"outputs":[],"source":["cfg = {\n","    'dataset_root': '../input/ml2022spring-hw13/food11-hw13',\n","    'save_dir': './outputs',\n","    'exp_name': \"simple_baseline\",\n","    'batch_size': 64,\n","    'lr': 5e-4,\n","    'seed': 20220013,\n","    'loss_fn_type': 'KD', # simple baseline: CE, medium baseline: KD.\n","    'weight_decay': 0, #1e-5,\n","    'grad_norm_max': 10,\n","    'n_epochs': 20, # 训练更多的步骤以通过中等基线(medium baseline).\n","    'patience': 300,\n","}"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:53:55.714689Z","iopub.status.busy":"2023-07-02T08:53:55.714253Z","iopub.status.idle":"2023-07-02T08:53:55.758886Z","shell.execute_reply":"2023-07-02T08:53:55.757953Z","shell.execute_reply.started":"2023-07-02T08:53:55.714650Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Set env random_seed = 20220013\n","{'dataset_root': '../input/ml2022spring-hw13/food11-hw13', 'save_dir': './outputs', 'exp_name': 'simple_baseline', 'batch_size': 64, 'lr': 0.0005, 'seed': 20220013, 'loss_fn_type': 'KD', 'weight_decay': 0, 'grad_norm_max': 10, 'n_epochs': 20, 'patience': 300}\n"]}],"source":["# 设置随机种子\n","all_seed(cfg['seed'])\n","save_path = os.path.join(cfg['save_dir'], cfg['exp_name'])\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path, exist_ok=True)\n","\n","log_path = f\"{save_path}/log.txt\"\n","if os.path.exists(log_path):\n","    os.system(f\"rm {log_path}\")\n","# 定义简单的日志方法\n","log_fw = open(log_path, 'a+') # 打开日志文件保存日志\n","def log(text):     # 定义一个日志记录函数来跟踪训练过程\n","    print(text)\n","    log_fw.write(str(text)+'\\n')\n","    log_fw.flush()\n","\n","log(cfg)  # 写入配置"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Dataset\n","\n","在本次作业中我们使用 Food11 数据集, 和homework3数据集相似，不过数据上稍微做了一些调整. 数据集可以直接在kaggle中载入，或者通过链接下载。\n","\n","```shell\n","# 从github下载数据 (大约 1.12G)\n","!wget https://github.com/virginiakm1988/ML2022-Spring/raw/main/HW13/food11-hw13.tar.gz\n","# 备份链接:\n","!wget https://github.com/andybi7676/ml2022spring-hw13/raw/main/food11-hw13.tar.gz -O food11-hw13.tar.gz\n","# !gdown '1ijKoNmpike_yjUw8SWRVVWVoMOXXqycj' --output food11-hw13.tar.gz\n","\n","# 解压\n","!tar -xzf ./food11-hw13.tar.gz \n","# !tar -xzvf ./food11-hw13.tar.gz # 可以查看解压进度\n","```\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:53:59.714815Z","iopub.status.busy":"2023-07-02T08:53:59.714463Z","iopub.status.idle":"2023-07-02T08:54:00.666385Z","shell.execute_reply":"2023-07-02T08:54:00.665172Z","shell.execute_reply.started":"2023-07-02T08:53:59.714788Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["evaluation  resnet18_teacher.ckpt  training  validation\n"]}],"source":["!ls ../input/ml2022spring-hw13/food11-hw13"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:54:00.669393Z","iopub.status.busy":"2023-07-02T08:54:00.668672Z","iopub.status.idle":"2023-07-02T08:54:24.047929Z","shell.execute_reply":"2023-07-02T08:54:24.046946Z","shell.execute_reply.started":"2023-07-02T08:54:00.669353Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["../input/ml2022spring-hw13/food11-hw13: 1 files.\n","../input/ml2022spring-hw13/food11-hw13/validation: 3430 files.\n","../input/ml2022spring-hw13/food11-hw13/training: 9866 files.\n","../input/ml2022spring-hw13/food11-hw13/evaluation: 3347 files.\n"]}],"source":["for dirname, _, filenames in os.walk('../input/ml2022spring-hw13/food11-hw13'):\n","    if len(filenames) > 0:\n","        print(f\"{dirname}: {len(filenames)} files.\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["下一步, 特殊的train/test数据集变换进行数据扩增  \n","Torchvision 提供了很多实用的图像预处理`image preprocessing`方法，数据扩增`data augmentation`方法\n","\n","可以参考 [PyTorch官方文档-transforms](https://pytorch.org/vision/stable/transforms.html) 了解不同的transforms方法。"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:54:24.050627Z","iopub.status.busy":"2023-07-02T08:54:24.049937Z","iopub.status.idle":"2023-07-02T08:54:24.058546Z","shell.execute_reply":"2023-07-02T08:54:24.057620Z","shell.execute_reply.started":"2023-07-02T08:54:24.050591Z"},"trusted":true},"outputs":[],"source":["normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","# 定义 training/testing transforms\n","test_tfm = transforms.Compose([\n","    # 如果你正在使用提供的教师模型(teacher model)，则不建议修改此部分。\n","    # 下列的transform 方法是标准的，并且足以进行测试。\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    normalize,\n","])\n","\n","train_tfm = transforms.Compose([\n","    # 在这里增加一些有用的transform 或 数据扩增方法, 基于你在HW3中学习的经验\n","    transforms.Resize(256),  # 你可以修改这里\n","    transforms.CenterCrop(224), # 你可以修改这里, 但是要注意，给定教师模型(teacher model)的输入大小是224。\n","    # 因此，除了224之外的输入大小可能会降低模型性能。需要注意。\n","    transforms.RandomHorizontalFlip(), # 你可以修改这里.\n","    transforms.ToTensor(),\n","    normalize,\n","])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:54:24.061232Z","iopub.status.busy":"2023-07-02T08:54:24.060415Z","iopub.status.idle":"2023-07-02T08:54:24.070649Z","shell.execute_reply":"2023-07-02T08:54:24.069680Z","shell.execute_reply.started":"2023-07-02T08:54:24.061200Z"},"trusted":true},"outputs":[],"source":["class FoodDataset(Dataset):\n","    def __init__(self, path, tfm=test_tfm, files=None):\n","        super().__init__()\n","        self.path = path\n","        self.files = sorted([os.path.join(path, i) for i in os.listdir(path) if i.endswith('.jpg')])\n","        if files is not None:\n","            self.files = files\n","        print(f'One {path} sample', self.files[0])\n","        self.tfm = tfm\n","    \n","    def __len__(self):\n","        return len(self.files)\n","    \n","    def __getitem__(self, idx):\n","        fname = self.files[idx]\n","        im = Image.open(fname)\n","        im = self.tfm(im)\n","        try:\n","            label = int(fname.split(\"/\")[-1].split('_')[0])\n","        except:\n","            label = -1\n","        return im, label\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:54:24.074229Z","iopub.status.busy":"2023-07-02T08:54:24.073868Z","iopub.status.idle":"2023-07-02T08:54:24.122475Z","shell.execute_reply":"2023-07-02T08:54:24.121571Z","shell.execute_reply.started":"2023-07-02T08:54:24.074195Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["One ../input/ml2022spring-hw13/food11-hw13/training sample ../input/ml2022spring-hw13/food11-hw13/training/0_0.jpg\n","One ../input/ml2022spring-hw13/food11-hw13/validation sample ../input/ml2022spring-hw13/food11-hw13/validation/0_0.jpg\n"]}],"source":["train_set = FoodDataset(os.path.join(cfg['dataset_root'], \"training\"), tfm=train_tfm)\n","train_loader = DataLoader(train_set,batch_size=cfg['batch_size'], shuffle=True, num_workers=0, pin_memory=True)\n","\n","val_set = FoodDataset(os.path.join(cfg['dataset_root'], \"validation\"), tfm=test_tfm)\n","val_loader = DataLoader(val_set, batch_size=cfg['batch_size'], shuffle=False, num_workers=0, pin_memory=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# &#x2728; Architecture_Design\n","\n","在这个作业中我们需要设计一个更小的网络，并使它表现的十分良好。显然，一个好的网络结构的设计是十分关键的。  \n","这里我们介绍深度`depthwise`和逐点`pointwise`卷积. 当涉及到网络压缩时， 这些变体的卷积架构设计是一些常见技术。\n","\n","- `depthwise`:\n","    - 一个kenerl对一个channel\n","    - in_channel == out_channel\n","    - 缺点：无法捕捉channel之间的关系\n","    \n","- `pointwise`:\n","    - `kernel_size=1`\n","    - 仅仅考虑channel之间的关系\n","    \n","- `depthwise` + `pointwise`\n","    - 参数减少 $\\frac{1}{O}+\\frac{1}{K\\times K}$ `O-输出channel, K-kernel大小`\n","\n","![dpdw](./HW13_pic/dwpw.png)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:54:24.124594Z","iopub.status.busy":"2023-07-02T08:54:24.123978Z","iopub.status.idle":"2023-07-02T08:54:24.130327Z","shell.execute_reply":"2023-07-02T08:54:24.129313Z","shell.execute_reply.started":"2023-07-02T08:54:24.124560Z"},"trusted":true},"outputs":[],"source":["# 示例：Depthwise and Pointwise Convlution\n","def dwpw_conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, groups=in_channels), # depthwise convolution\n","        nn.Conv2d(in_channels, out_channels, 1) # pointwise convolution\n","    )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- 其他有用的方法\n","    - [group convolution](https://www.researchgate.net/figure/The-transformations-within-a-layer-in-DenseNets-left-and-CondenseNets-at-training-time_fig2_321325862)(实际上`depthwise convolution`是一种特殊的`group convolution`)\n","    - [SqueezeNet](https://arxiv.org/abs/1602.07360)\n","    - [MobileNet](https://arxiv.org/abs/1704.04861)\n","    - [ShuffleNet](https://arxiv.org/abs/1707.01083)\n","    - [Xception](https://arxiv.org/abs/1610.02357)\n","    - [GhostNet](https://arxiv.org/abs/1911.11907)\n"," \n","在介绍了深度卷积和点卷积之后，让我们定义**学生网络`student network`框架**。在这里，我们有一个由一些具有深度和逐点卷积的规则卷积层形成的简单网络。通过这种方式，你可以进一步增加网络的深度或宽度。\n","\n","<font color=darkred><b>TODO：修改成自己的网络框架</font></b>   "]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:54:24.132299Z","iopub.status.busy":"2023-07-02T08:54:24.131962Z","iopub.status.idle":"2023-07-02T08:54:24.148079Z","shell.execute_reply":"2023-07-02T08:54:24.147009Z","shell.execute_reply.started":"2023-07-02T08:54:24.132269Z"},"trusted":true},"outputs":[],"source":["# 在这里定义自己的 student network.\n","# 我们将使用你的student network来评估您的结果（包括总参数量）\n","\n","class StudentNetOrg(nn.Module):\n","    def __init__(self):\n","        super(StudentNetOrg, self).__init__()\n","        # TODO: 修改成自己的网络框架\n","        self.cnn = nn.Sequential(\n","            nn.Conv2d(3, 32, 3),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 32, 3),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","            \n","            nn.Conv2d(32, 64, 3),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","            \n","            nn.Conv2d(64, 100, 3),\n","            nn.BatchNorm2d(100),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","            \n","            # 在这里，我们对各种输入大小采用全局平均。\n","            nn.AdaptiveAvgPool2d((1, 1))\n","        )\n","        self.fc = nn.Sequential(\n","            nn.Linear(100, 11)\n","        )\n","\n","    def forward(self, x):\n","        out = self.cnn(x)\n","        out = out.view(out.size()[0], -1)\n","        return self.fc(out)\n","\n","\n","class StudentNet(nn.Module):\n","    def __init__(self):\n","        super(StudentNet, self).__init__()\n","        # TODO: 修改成自己的网络框架\n","        self.cnn = nn.Sequential(\n","            dwpw_conv(3, 32, 3, stride=1, padding=0),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            dwpw_conv(32, 32, 3, stride=1, padding=0),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","            \n","            dwpw_conv(32, 64, 3, stride=1, padding=0),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","            \n","            dwpw_conv(64, 100, 3, stride=1, padding=0),\n","            nn.BatchNorm2d(100),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","            \n","            # 在这里，我们对各种输入大小采用全局平均。\n","            nn.AdaptiveAvgPool2d((1, 1))\n","        )\n","        self.fc = nn.Sequential(\n","            nn.Linear(100, 11)\n","        )\n","\n","    def forward(self, x):\n","        out = self.cnn(x)\n","        out = out.view(out.size()[0], -1)\n","        return self.fc(out)\n","\n","\n","def get_student_model():\n","    return StudentNet()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["确定`student network`框架后, 需要使用`torchsummary`获取网络的信息和验证参数总数. 需要注意`student network`网络参数总量，  \n","网络参数的总量不能超过限制(`总参数（torchsummary中展示）<=100,000`)."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:54:24.150381Z","iopub.status.busy":"2023-07-02T08:54:24.149340Z","iopub.status.idle":"2023-07-02T08:54:24.624424Z","shell.execute_reply":"2023-07-02T08:54:24.622209Z","shell.execute_reply.started":"2023-07-02T08:54:24.150348Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["**********************************************************************\n","[ StudentNetOrg ]\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 222, 222]             896\n","       BatchNorm2d-2         [-1, 32, 222, 222]              64\n","              ReLU-3         [-1, 32, 222, 222]               0\n","            Conv2d-4         [-1, 32, 220, 220]           9,248\n","       BatchNorm2d-5         [-1, 32, 220, 220]              64\n","              ReLU-6         [-1, 32, 220, 220]               0\n","         MaxPool2d-7         [-1, 32, 110, 110]               0\n","            Conv2d-8         [-1, 64, 108, 108]          18,496\n","       BatchNorm2d-9         [-1, 64, 108, 108]             128\n","             ReLU-10         [-1, 64, 108, 108]               0\n","        MaxPool2d-11           [-1, 64, 54, 54]               0\n","           Conv2d-12          [-1, 100, 52, 52]          57,700\n","      BatchNorm2d-13          [-1, 100, 52, 52]             200\n","             ReLU-14          [-1, 100, 52, 52]               0\n","        MaxPool2d-15          [-1, 100, 26, 26]               0\n","AdaptiveAvgPool2d-16            [-1, 100, 1, 1]               0\n","           Linear-17                   [-1, 11]           1,111\n","================================================================\n","Total params: 87,907\n","Trainable params: 87,907\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 99.72\n","Params size (MB): 0.34\n","Estimated Total Size (MB): 100.62\n","----------------------------------------------------------------\n","\n","\n","**********************************************************************\n","[ StudentNet ]\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 3, 222, 222]              30\n","            Conv2d-2         [-1, 32, 222, 222]             128\n","       BatchNorm2d-3         [-1, 32, 222, 222]              64\n","              ReLU-4         [-1, 32, 222, 222]               0\n","            Conv2d-5         [-1, 32, 220, 220]             320\n","            Conv2d-6         [-1, 32, 220, 220]           1,056\n","       BatchNorm2d-7         [-1, 32, 220, 220]              64\n","              ReLU-8         [-1, 32, 220, 220]               0\n","         MaxPool2d-9         [-1, 32, 110, 110]               0\n","           Conv2d-10         [-1, 32, 108, 108]             320\n","           Conv2d-11         [-1, 64, 108, 108]           2,112\n","      BatchNorm2d-12         [-1, 64, 108, 108]             128\n","             ReLU-13         [-1, 64, 108, 108]               0\n","        MaxPool2d-14           [-1, 64, 54, 54]               0\n","           Conv2d-15           [-1, 64, 52, 52]             640\n","           Conv2d-16          [-1, 100, 52, 52]           6,500\n","      BatchNorm2d-17          [-1, 100, 52, 52]             200\n","             ReLU-18          [-1, 100, 52, 52]               0\n","        MaxPool2d-19          [-1, 100, 26, 26]               0\n","AdaptiveAvgPool2d-20            [-1, 100, 1, 1]               0\n","           Linear-21                   [-1, 11]           1,111\n","================================================================\n","Total params: 12,673\n","Trainable params: 12,673\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 116.83\n","Params size (MB): 0.05\n","Estimated Total Size (MB): 117.45\n","----------------------------------------------------------------\n"]}],"source":["student_model= get_student_model()\n","student_model_org = StudentNetOrg()\n","print('**'*35)\n","print(\"[ StudentNetOrg ]\")\n","summary(student_model_org,  (3, 224, 224), device='cpu')\n","print(\"\\n\")\n","print('**'*35)\n","print(\"[ StudentNet ]\")\n","summary(student_model, (3, 224, 224), device='cpu')"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:54:24.626374Z","iopub.status.busy":"2023-07-02T08:54:24.625888Z","iopub.status.idle":"2023-07-02T08:54:27.024117Z","shell.execute_reply":"2023-07-02T08:54:27.023119Z","shell.execute_reply.started":"2023-07-02T08:54:24.626337Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","              ReLU-7           [-1, 64, 56, 56]               0\n","            Conv2d-8           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-9           [-1, 64, 56, 56]             128\n","             ReLU-10           [-1, 64, 56, 56]               0\n","       BasicBlock-11           [-1, 64, 56, 56]               0\n","           Conv2d-12           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-13           [-1, 64, 56, 56]             128\n","             ReLU-14           [-1, 64, 56, 56]               0\n","           Conv2d-15           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-16           [-1, 64, 56, 56]             128\n","             ReLU-17           [-1, 64, 56, 56]               0\n","       BasicBlock-18           [-1, 64, 56, 56]               0\n","           Conv2d-19          [-1, 128, 28, 28]          73,728\n","      BatchNorm2d-20          [-1, 128, 28, 28]             256\n","             ReLU-21          [-1, 128, 28, 28]               0\n","           Conv2d-22          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-23          [-1, 128, 28, 28]             256\n","           Conv2d-24          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-25          [-1, 128, 28, 28]             256\n","             ReLU-26          [-1, 128, 28, 28]               0\n","       BasicBlock-27          [-1, 128, 28, 28]               0\n","           Conv2d-28          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-29          [-1, 128, 28, 28]             256\n","             ReLU-30          [-1, 128, 28, 28]               0\n","           Conv2d-31          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-32          [-1, 128, 28, 28]             256\n","             ReLU-33          [-1, 128, 28, 28]               0\n","       BasicBlock-34          [-1, 128, 28, 28]               0\n","           Conv2d-35          [-1, 256, 14, 14]         294,912\n","      BatchNorm2d-36          [-1, 256, 14, 14]             512\n","             ReLU-37          [-1, 256, 14, 14]               0\n","           Conv2d-38          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-39          [-1, 256, 14, 14]             512\n","           Conv2d-40          [-1, 256, 14, 14]          32,768\n","      BatchNorm2d-41          [-1, 256, 14, 14]             512\n","             ReLU-42          [-1, 256, 14, 14]               0\n","       BasicBlock-43          [-1, 256, 14, 14]               0\n","           Conv2d-44          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-45          [-1, 256, 14, 14]             512\n","             ReLU-46          [-1, 256, 14, 14]               0\n","           Conv2d-47          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-48          [-1, 256, 14, 14]             512\n","             ReLU-49          [-1, 256, 14, 14]               0\n","       BasicBlock-50          [-1, 256, 14, 14]               0\n","           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n","      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n","             ReLU-53            [-1, 512, 7, 7]               0\n","           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n","           Conv2d-56            [-1, 512, 7, 7]         131,072\n","      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n","             ReLU-58            [-1, 512, 7, 7]               0\n","       BasicBlock-59            [-1, 512, 7, 7]               0\n","           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n","             ReLU-62            [-1, 512, 7, 7]               0\n","           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n","             ReLU-65            [-1, 512, 7, 7]               0\n","       BasicBlock-66            [-1, 512, 7, 7]               0\n","AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n","           Linear-68                   [-1, 11]           5,643\n","================================================================\n","Total params: 11,182,155\n","Trainable params: 11,182,155\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 62.79\n","Params size (MB): 42.66\n","Estimated Total Size (MB): 106.02\n","----------------------------------------------------------------\n"]}],"source":["# 载入提供的教师模型 (restnet18 num_classes=11, test-acc ~= 89.9%)\n","teacher_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=11)\n","# load_state dict\n","teach_ckpt_path = os.path.join(cfg['dataset_root'], \"resnet18_teacher.ckpt\")\n","teacher_model.load_state_dict(torch.load(teach_ckpt_path, map_location='cpu'))\n","summary(teacher_model, (3, 224, 224), device='cpu')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#   &#x2728; Knowledge_Distillation\n","\n","既然我们有一个学习过的大模型，那就让它教另一个小模型吧。在实现中，让训练目标是大模型的预测，而不是实际标签\n","\n","\n","**为什么这样能有效训练出小网络 ?**\n","- 如果数据干净，那么大模型的预测可能会忽略带有错误标记的数据的噪声\n","- 类之间可能存在一些关系，因此教师模型中的软标签可能会很有用。例如，数字8与6、9、0比1、7更相似\n","\n","**如何实施训练 ?**\n","- 损失函数定义\n","$$\\text{Loss} = \\alpha T^2 \\times KL(p||q) + (1-\\alpha)\\text{(Original Cross Entropy Loss)}$$\n","\n","    - $\\text{where p=softmax}(\\frac{\\text{student's logits}}{T})$\n","    - $\\text{where q=softmax}(\\frac{\\text{teacher's logits}}{T})$\n","    \n","    \n","- 使用链接: [`pytorch docs of KLDivLoss with examples` Link](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html)\n","- 原始论文: [`Distilling the Knowledge in a Neural Network` Link](https://arxiv.org/abs/1503.02531)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=darkred><b>TODO：参考上述的函数，结合`KL divergence Loss`和`CE Loss`完成损失函数的定义</font></b>    "]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:54:27.026540Z","iopub.status.busy":"2023-07-02T08:54:27.025455Z","iopub.status.idle":"2023-07-02T08:54:27.049734Z","shell.execute_reply":"2023-07-02T08:54:27.048306Z","shell.execute_reply.started":"2023-07-02T08:54:27.026506Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([ 0.3593,  1.4295, -1.2391, -0.6948])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["a = torch.randn(4)\n","a"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:54:27.053416Z","iopub.status.busy":"2023-07-02T08:54:27.052945Z","iopub.status.idle":"2023-07-02T08:54:27.064340Z","shell.execute_reply":"2023-07-02T08:54:27.063286Z","shell.execute_reply.started":"2023-07-02T08:54:27.053388Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(tensor([0.2239, 0.6528, 0.0453, 0.0780]),\n"," tensor([0.2390, 0.6060, 0.0595, 0.0955]),\n"," tensor([0.1035, 0.8797, 0.0042, 0.0126]))"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["sft = nn.Softmax(dim=-1)\n","sft(a), sft(a/1.15), sft(a/0.5), "]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:54:44.608021Z","iopub.status.busy":"2023-07-02T08:54:44.607636Z","iopub.status.idle":"2023-07-02T08:54:44.616718Z","shell.execute_reply":"2023-07-02T08:54:44.613899Z","shell.execute_reply.started":"2023-07-02T08:54:44.607991Z"},"trusted":true},"outputs":[],"source":["# 利用 KL divergence loss 实现知识蒸馏(know distillation)的损失函数 \n","def loss_fn_kd(student_logits, labels, teacher_logits, alpha=0.5, temperature=1.15):\n","    # temperature 越大越平滑\n","    # TODO: \n","    kl_loss = torch.nn.KLDivLoss(reduction='mean', log_target=True)\n","    ce_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n","    sft = nn.Softmax(dim=-1)\n","    return alpha * temperature * temperature * kl_loss(sft(student_logits/temperature), sft(teacher_logits/temperature)) \\\n","            + (1-alpha) * ce_loss(student_logits, labels)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:55:06.470057Z","iopub.status.busy":"2023-07-02T08:55:06.469656Z","iopub.status.idle":"2023-07-02T08:55:06.478017Z","shell.execute_reply":"2023-07-02T08:55:06.476881Z","shell.execute_reply.started":"2023-07-02T08:55:06.470024Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cfg['loss_fn_type']= KD\n","device: cuda\n"]}],"source":["print(\"cfg['loss_fn_type']=\", cfg['loss_fn_type'])\n","#  选择损失函数\n","if cfg['loss_fn_type'] == 'CE':\n","    loss_fn = nn.CrossEntropyLoss() # simple base line\n","\n","if cfg['loss_fn_type'] == 'KD':\n","    loss_fn = loss_fn_kd\n","\n","# 还可以自定义一些其他方法\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","log(f'device: {device}')\n","device = torch.device(device)\n","n_epochs = cfg['n_epochs']\n","patience = cfg['patience']"]},{"attachments":{},"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-06-29T02:38:20.342506Z","iopub.status.busy":"2023-06-29T02:38:20.342047Z","iopub.status.idle":"2023-06-29T02:38:20.350732Z","shell.execute_reply":"2023-06-29T02:38:20.349126Z","shell.execute_reply.started":"2023-06-29T02:38:20.342474Z"}},"source":["#   &#x2728; Training\n","\n","实现简单基线的训练循环，可以随意修改。"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T08:55:21.185079Z","iopub.status.busy":"2023-07-02T08:55:21.184692Z","iopub.status.idle":"2023-07-02T09:45:07.856980Z","shell.execute_reply":"2023-07-02T09:45:07.856043Z","shell.execute_reply.started":"2023-07-02T08:55:21.185046Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 001 / 020 ]:   0%|          | 0/155 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\n","[ Train | Epoch 001 / 020 ]: 100%|██████████| 155/155 [03:01<00:00,  1.17s/it, loss=64.1]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 001 / 020 ] loss = 1.14215 acc = 0.26748\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 001 / 020 ]: 100%|██████████| 54/54 [00:55<00:00,  1.04s/it, loss=58.3]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 001 / 020 ] loss = 1.07294 acc = 0.30671\n","Best model found at epoch 1. saving model. acc=0.30671\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 002 / 020 ]: 100%|██████████| 155/155 [01:52<00:00,  1.38it/s, loss=58.6]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 002 / 020 ] loss = 1.04547 acc = 0.32962\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 002 / 020 ]: 100%|██████████| 54/54 [00:33<00:00,  1.59it/s, loss=48.9]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 002 / 020 ] loss = 0.99463 acc = 0.34898\n","Best model found at epoch 2. saving model. acc=0.34898\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 003 / 020 ]: 100%|██████████| 155/155 [01:53<00:00,  1.37it/s, loss=57.4]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 003 / 020 ] loss = 0.98765 acc = 0.36611\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 003 / 020 ]: 100%|██████████| 54/54 [00:32<00:00,  1.65it/s, loss=51.4]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 003 / 020 ] loss = 0.94953 acc = 0.38426\n","Best model found at epoch 3. saving model. acc=0.38426\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 004 / 020 ]: 100%|██████████| 155/155 [01:51<00:00,  1.39it/s, loss=56.5]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 004 / 020 ] loss = 0.95666 acc = 0.38790\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 004 / 020 ]: 100%|██████████| 54/54 [00:32<00:00,  1.66it/s, loss=47.2]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 004 / 020 ] loss = 0.93078 acc = 0.39621\n","Best model found at epoch 4. saving model. acc=0.39621\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 005 / 020 ]: 100%|██████████| 155/155 [01:50<00:00,  1.40it/s, loss=53.3]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 005 / 020 ] loss = 0.93707 acc = 0.40087\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 005 / 020 ]: 100%|██████████| 54/54 [00:33<00:00,  1.60it/s, loss=48.5]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 005 / 020 ] loss = 0.92430 acc = 0.40466\n","Best model found at epoch 5. saving model. acc=0.40466\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 006 / 020 ]: 100%|██████████| 155/155 [01:52<00:00,  1.37it/s, loss=52]  \n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 006 / 020 ] loss = 0.91938 acc = 0.41324\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 006 / 020 ]: 100%|██████████| 54/54 [00:32<00:00,  1.64it/s, loss=48.1]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 006 / 020 ] loss = 0.90715 acc = 0.40933\n","Best model found at epoch 6. saving model. acc=0.40933\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 007 / 020 ]: 100%|██████████| 155/155 [01:49<00:00,  1.41it/s, loss=53.3]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 007 / 020 ] loss = 0.90614 acc = 0.42986\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 007 / 020 ]: 100%|██████████| 54/54 [00:32<00:00,  1.66it/s, loss=44.9]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 007 / 020 ] loss = 0.89735 acc = 0.42478\n","Best model found at epoch 7. saving model. acc=0.42478\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 008 / 020 ]: 100%|██████████| 155/155 [01:51<00:00,  1.39it/s, loss=52.2]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 008 / 020 ] loss = 0.89150 acc = 0.43929\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 008 / 020 ]: 100%|██████████| 54/54 [00:32<00:00,  1.65it/s, loss=45.8]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 008 / 020 ] loss = 0.88969 acc = 0.42566\n","Best model found at epoch 8. saving model. acc=0.42566\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 009 / 020 ]: 100%|██████████| 155/155 [01:52<00:00,  1.37it/s, loss=52]  \n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 009 / 020 ] loss = 0.88218 acc = 0.44679\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 009 / 020 ]: 100%|██████████| 54/54 [00:33<00:00,  1.60it/s, loss=46.1]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 009 / 020 ] loss = 0.89728 acc = 0.41166\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 010 / 020 ]: 100%|██████████| 155/155 [01:52<00:00,  1.38it/s, loss=51.2]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 010 / 020 ] loss = 0.87277 acc = 0.44810\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 010 / 020 ]: 100%|██████████| 54/54 [00:33<00:00,  1.62it/s, loss=45]  \n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 010 / 020 ] loss = 0.86701 acc = 0.45248\n","Best model found at epoch 10. saving model. acc=0.45248\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 011 / 020 ]: 100%|██████████| 155/155 [01:53<00:00,  1.37it/s, loss=49.9]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 011 / 020 ] loss = 0.86178 acc = 0.45885\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 011 / 020 ]: 100%|██████████| 54/54 [00:32<00:00,  1.64it/s, loss=40.9]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 011 / 020 ] loss = 0.89583 acc = 0.41691\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 012 / 020 ]: 100%|██████████| 155/155 [01:54<00:00,  1.35it/s, loss=50.7]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 012 / 020 ] loss = 0.85288 acc = 0.46432\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 012 / 020 ]: 100%|██████████| 54/54 [00:32<00:00,  1.64it/s, loss=41.5]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 012 / 020 ] loss = 0.84904 acc = 0.46793\n","Best model found at epoch 12. saving model. acc=0.46793\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 013 / 020 ]: 100%|██████████| 155/155 [01:50<00:00,  1.40it/s, loss=49]  \n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 013 / 020 ] loss = 0.84606 acc = 0.46564\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 013 / 020 ]: 100%|██████████| 54/54 [00:32<00:00,  1.66it/s, loss=44.1]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 013 / 020 ] loss = 0.84091 acc = 0.47318\n","Best model found at epoch 13. saving model. acc=0.47318\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 014 / 020 ]: 100%|██████████| 155/155 [01:51<00:00,  1.39it/s, loss=49.2]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 014 / 020 ] loss = 0.84040 acc = 0.47486\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 014 / 020 ]: 100%|██████████| 54/54 [00:32<00:00,  1.65it/s, loss=42]  \n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 014 / 020 ] loss = 0.85895 acc = 0.44927\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 015 / 020 ]: 100%|██████████| 155/155 [01:51<00:00,  1.39it/s, loss=48.7]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 015 / 020 ] loss = 0.83053 acc = 0.48034\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 015 / 020 ]: 100%|██████████| 54/54 [00:32<00:00,  1.65it/s, loss=48.6]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 015 / 020 ] loss = 0.83502 acc = 0.48105\n","Best model found at epoch 15. saving model. acc=0.48105\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 016 / 020 ]: 100%|██████████| 155/155 [01:50<00:00,  1.40it/s, loss=48.8]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 016 / 020 ] loss = 0.81940 acc = 0.49159\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 016 / 020 ]: 100%|██████████| 54/54 [00:32<00:00,  1.65it/s, loss=41.3]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 016 / 020 ] loss = 0.82605 acc = 0.47901\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 017 / 020 ]: 100%|██████████| 155/155 [01:50<00:00,  1.40it/s, loss=49.1]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 017 / 020 ] loss = 0.81488 acc = 0.49220\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 017 / 020 ]: 100%|██████████| 54/54 [00:32<00:00,  1.64it/s, loss=39.8]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 017 / 020 ] loss = 0.82721 acc = 0.46939\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 018 / 020 ]: 100%|██████████| 155/155 [01:50<00:00,  1.40it/s, loss=46.7]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 018 / 020 ] loss = 0.80636 acc = 0.49402\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 018 / 020 ]: 100%|██████████| 54/54 [00:32<00:00,  1.67it/s, loss=44.3]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 018 / 020 ] loss = 0.82338 acc = 0.48338\n","Best model found at epoch 18. saving model. acc=0.48338\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 019 / 020 ]: 100%|██████████| 155/155 [01:49<00:00,  1.41it/s, loss=47.3]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 019 / 020 ] loss = 0.80213 acc = 0.49868\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 019 / 020 ]: 100%|██████████| 54/54 [00:31<00:00,  1.69it/s, loss=39.2]\n"]},{"name":"stdout","output_type":"stream","text":["[ Val | 019 / 020 ] loss = 0.81097 acc = 0.48630\n","Best model found at epoch 19. saving model. acc=0.48630\n"]},{"name":"stderr","output_type":"stream","text":["[ Train | Epoch 020 / 020 ]: 100%|██████████| 155/155 [01:50<00:00,  1.40it/s, loss=46.4]\n"]},{"name":"stdout","output_type":"stream","text":["[ Train | 020 / 020 ] loss = 0.79294 acc = 0.51074\n"]},{"name":"stderr","output_type":"stream","text":["[ Val | Epoch 020 / 020 ]: 100%|██████████| 54/54 [00:33<00:00,  1.63it/s, loss=42.2]"]},{"name":"stdout","output_type":"stream","text":["[ Val | 020 / 020 ] loss = 0.81347 acc = 0.49446\n","Best model found at epoch 20. saving model. acc=0.49446\n","Finish training\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# 模型初始化，并将参数移入训练设备\n","student_model.to(device)\n","opt = torch.optim.Adam(student_model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n","\n","# 初始化跟踪器, 这部分不是训练参数不需要修改\n","stale = 0\n","best_acc = 0.0\n","\n","teacher_model.to(device)\n","teacher_model.eval() # MEDIUM BASELINE\n","for epoch in range(n_epochs):\n","    # ---------------- Training ----------------------\n","    # 在训练之前，确保模型是开启训练模式的\n","    student_model.train()\n","    # 记录训练过程的信息\n","    train_loss = []\n","    train_accs = []\n","    train_lens = []\n","    tq_bar = tqdm(train_loader)\n","    tq_bar.set_description(f\"[ Train | Epoch {epoch+1:03d} / {n_epochs:03d} ]\")\n","    for imgs, labels in tq_bar:\n","        imgs = imgs.to(device)\n","        labels = labels.to(device)\n","#         imgs = imgs.half() # 开启半精度。直接可以加快运行速度、减少GPU占用，并且只有不明显的accuracy损失。\n","        # 前向传播\n","        with torch.no_grad():  # MEDIUM BASELINE\n","            teacher_logits = teacher_model(imgs)  # MEDIUM BASELINE\n","        logits = student_model(imgs)\n","        # 计算损失.\n","        loss = loss_fn(logits, labels, teacher_logits)\n","#         loss = loss_fn(logits, labels)\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","        acc = (logits.argmax(dim=-1) == labels).float().sum()\n","        # 记录 loss 和 accuracy.\n","        batch_len = len(imgs)\n","        train_loss.append(loss.cpu().item() * batch_len)\n","        train_accs.append(acc)\n","        train_lens.append(batch_len)\n","        tq_bar.set_postfix({\"loss\" : np.mean(train_loss[-10:])})\n","    \n","    train_loss = sum(train_loss)/sum(train_lens)\n","    train_acc = sum(train_accs)/sum(train_lens)\n","    # 打印信息\n","    log(f'[ Train | {epoch+1:03d} / {n_epochs:03d} ] loss = {train_loss:.5f} acc = {train_acc:.5f}')\n","    # ---------------- validation ----------------------\n","    student_model.eval()\n","    val_loss = []\n","    val_accs = []\n","    val_lens = []\n","    tq_bar = tqdm(val_loader)\n","    tq_bar.set_description(f\"[ Val | Epoch {epoch+1:03d} / {n_epochs:03d} ]\")\n","    for imgs, labels in tq_bar:\n","        imgs = imgs.to(device)\n","        labels = labels.to(device)\n","        # 前向传播\n","        with torch.no_grad():  # MEDIUM BASELINE\n","            teacher_logits = teacher_model(imgs)  # MEDIUM BASELINE\n","        with torch.no_grad():\n","            logits = student_model(imgs)\n","        loss = loss_fn(logits, labels, teacher_logits)\n","#         loss = loss_fn(logits, labels)\n","        acc = (logits.argmax(dim=-1) == labels).float().sum()\n","        # 记录 loss 和 accuracy.\n","        batch_len = len(imgs)\n","        val_loss.append(loss.cpu().item() * batch_len)\n","        val_accs.append(acc)\n","        val_lens.append(batch_len)\n","        tq_bar.set_postfix({\"loss\" : np.mean(val_loss[-10:])})\n","    \n","    val_loss = sum(val_loss)/sum(val_lens)\n","    val_acc = sum(val_accs)/sum(val_lens)\n","    log(f'[ Val | {epoch+1:03d} / {n_epochs:03d} ] loss = {val_loss:.5f} acc = {val_acc:.5f}')\n","    # 更新logs\n","    if val_acc > best_acc:\n","        log(f'Best model found at epoch {epoch+1}. saving model. acc={val_acc:.5f}')\n","        best_acc = val_acc\n","        torch.save(student_model.state_dict(), f\"{save_path}/student_best.ckpt\")\n","        stale = 0\n","    else:\n","        stale += 1\n","        if (stale > patience):\n","            log(f'No improving {patience} consecutions. early stopping')\n","            break\n","    \n","log(\"Finish training\")\n","log_fw.close()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Inference\n","载入训练好的最佳模型进行预测并生成`submission.csv`"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:45:07.859631Z","iopub.status.busy":"2023-07-02T09:45:07.859049Z","iopub.status.idle":"2023-07-02T09:45:07.876108Z","shell.execute_reply":"2023-07-02T09:45:07.875174Z","shell.execute_reply.started":"2023-07-02T09:45:07.859597Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["One ../input/ml2022spring-hw13/food11-hw13/evaluation sample ../input/ml2022spring-hw13/food11-hw13/evaluation/0000.jpg\n"]}],"source":["eval_set = FoodDataset(os.path.join(cfg['dataset_root'], \"evaluation\"), tfm=test_tfm)\n","eval_loader = DataLoader(eval_set, batch_size=cfg['batch_size'], shuffle=False, num_workers=0, pin_memory=True)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-07-02T09:45:07.877946Z","iopub.status.busy":"2023-07-02T09:45:07.877366Z","iopub.status.idle":"2023-07-02T09:45:59.518547Z","shell.execute_reply":"2023-07-02T09:45:59.517174Z","shell.execute_reply.started":"2023-07-02T09:45:07.877894Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 53/53 [00:51<00:00,  1.03it/s]\n"]}],"source":["# 载入模型\n","student_model_best = get_student_model()\n","ckpt_path = f\"{save_path}/student_best.ckpt\" \n","student_model_best.load_state_dict(torch.load(ckpt_path, map_location='cpu'))\n","student_model_best.to(device) \n","\n","# 开始评估\n","student_model_best.eval()\n","eval_preds = [] # storing predictions of the evaluation dataset\n","\n","for imgs, _ in tqdm(eval_loader):\n","    # 在eval中不需要进行梯度下降\n","    with torch.no_grad():\n","        logits = student_model_best(imgs.to(device))\n","        preds = list(logits.argmax(dim=-1).squeeze().cpu().numpy())\n","\n","    eval_preds += preds\n","\n","def pad4(i):\n","    return str(i).zfill(4)\n","\n","# 保存结果\n","ids = [pad4(i) for i in range(0, len(eval_set))]\n","categories = eval_preds\n","\n","df = pd.DataFrame()\n","df['Id'] = ids\n","df['Category'] = categories\n","df.to_csv(f\"submission.csv\", index=False) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
